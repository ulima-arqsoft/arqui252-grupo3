> [0. Acerca del Grupo](../../0.md) ‚Ä∫ [0.8. Temas Individuales (Parte 2)](../0.8.md) ‚Ä∫ [0.8.5. Integrante 5](0.8.5.md)

# 0.8.5. Integrante 5

Despliegue de LLMS en entornos locales (comparativa)
Autor: Alvaro Salazar Carpio| Fecha: Noviembre 2025 | Tecnolog√≠a: Python, Ollama, Streamlit

¬øQu√© es una IA Local?

El despliegue de LLMs en entornos locales consiste en ejecutar modelos de inteligencia artificial directamente en el hardware del usuario (CPU/GPU) sin necesidad de conexi√≥n a internet ni de enviar datos a servidores externos.

Esto contrasta con los servicios en la nube, ofreciendo tres ventajas clave:

Privacidad: Los datos nunca salen del ordenador.
Coste: La inferencia es gratuita (solo cuesta electricidad).
Disponibilidad: Funciona 100% offline.

Ejemplos de IAs Locales
Algunos de los modelos m√°s destacados son:

LLaMA (Meta): El est√°ndar de la industria abierta.

Mistral (Mistral AI): Modelos europeos optimizados para eficiencia.

Falcon: Modelos de alto rendimiento de los EAU.

BLOOM: El modelo multiling√ºe y de ciencia abierta por excelencia.

2. OLLAMA y Modelos Seleccionados
¬øQu√© es OLLAMA?
OLLAMA es el motor de ejecuci√≥n elegido para este proyecto. Act√∫a como una capa de abstracci√≥n que permite correr modelos de manera eficiente en sistemas operativos est√°ndar, gestionando la carga en memoria y la comunicaci√≥n v√≠a API local.

Modelos Comparados
Para esta demo se han seleccionado tres modelos de la categor√≠a "7B" (aprox. 7 billones de par√°metros), que representan el equilibrio ideal entre rendimiento y consumo para hardware de consumo:

-Llama 3 (Meta): La √∫ltima generaci√≥n de Meta, conocida por su razonamiento l√≥gico superior.
-Mistral (Mistral AI): Un modelo "denso" muy popular por su rapidez y capacidad de seguir instrucciones.
-Gemma:7b (Google): La apuesta abierta de Google, basada en la misma arquitectura que su modelo Gemini.

3. Preparaci√≥n del Ambiente 
REPO: https://github.com/salcar420/repo-demo-LLM.git

Para replicar este experimento, se configur√≥ un entorno de desarrollo en Python con las siguientes herramientas:

A. Instalaci√≥n del Motor
a)Descarga e instalaci√≥n de OLLAMA desde su web oficial.
b)Descarga de los modelos ("pesos") mediante terminal:
ollama pull llama3
ollama pull mistral
ollama pull gemma:7b

B. Entorno Python
Se utiliz√≥ Visual Studio Code como IDE. Las dependencias se gestionaron v√≠a pip:

Librer√≠as de Backend:

ollama: Para conectar Python con el servidor local de IA.
psutil: Para monitorear el consumo de RAM y CPU en tiempo real.
csv: Para el almacenamiento persistente de los resultados.

Librer√≠as de Frontend y Datos:

pandas: Para el an√°lisis y manipulaci√≥n de los datos obtenidos.
plotly: Para la generaci√≥n de gr√°ficos interactivos.
streamlit: Para el despliegue del Dashboard visual.


4. L√≥gica y Metodolog√≠a de Medici√≥n
El script de benchmarking fue dise√±ado para someter a los modelos a pruebas de estr√©s controladas.

Par√°metros Evaluados (KPIs)
Para determinar qu√© modelo es "mejor", medimos cuatro variables objetivas:

-TTFT: Latencia. El tiempo que tarda el modelo en empezar a escribir. Crucial para la sensaci√≥n de fluidez.
-TPS: Velocidad de generaci√≥n. Mide la potencia bruta de procesamiento.
-Consumo de RAM: Diferencial de memoria utilizada durante la inferencia (medido con psutil).
-Tiempo Total: Duraci√≥n completa de la tarea.


Estrategia de Prompts
Para evitar sesgos, no se us√≥ una sola pregunta. Se dise√±aron prompts divididos en 4 categor√≠as:
-Creatividad: Poemas y escritura libre.
-Razonamiento: Problemas de l√≥gica matem√°tica.
-Coding: Generaci√≥n de c√≥digo Python.
-Resumen: Capacidad de s√≠ntesis de textos.

<img width="1008" height="374" alt="image" src="https://github.com/user-attachments/assets/f6118448-3af8-43fb-b1f5-831701983004" />


5. Resultados y An√°lisis 
Basado en la ejecuci√≥n del benchmark en el entorno local, se obtuvieron los siguientes resultados comparativos entre Llama3, Mistral y Gemma:7b:

Dieron los siguientes resultados:
<img width="1565" height="469" alt="image" src="https://github.com/user-attachments/assets/70b07699-ba7d-4c73-bc7b-547437c82cee" />

<img width="1527" height="227" alt="image" src="https://github.com/user-attachments/assets/f295c029-48c6-44f2-bedf-6ae600aa6c73" />


A. Campe√≥n de Velocidad 
El modelo Mistral demostr√≥ ser el m√°s r√°pido indiscutible en este hardware.

<img width="1572" height="491" alt="image" src="https://github.com/user-attachments/assets/f6eb8199-62d1-4b9b-8407-e0362404f4ad" />


Velocidad Promedio: Alcanz√≥ unos  6 tokens/segundo de forma consistente en tareas de Razonamiento y Coding.

Comparativa: Super√≥ ligeramente a Llama3 (que rond√≥ los 5.1 - 5.7 t/s) y dej√≥ muy atr√°s a Gemma:7b, que sufri√≥ bastante en tareas creativas bajando a 1.58 t/s.

B. Latencia y "Arranque en Fr√≠o" 
<img width="780" height="513" alt="image" src="https://github.com/user-attachments/assets/9c06b0f1-2a2b-485b-83a6-2a0e6bb10c6b" />

Se observ√≥ un fen√≥meno t√©cnico importante llamado "Cold Start" :

Carga inicial: Todos los modelos tardaron entre 19 y 26 segundos en responder a la primera pregunta (Creatividad) mientras se cargaban en la memoria RAM.

Respuesta en Caliente: Una vez cargados, Mistral fue el m√°s √°gil con una latencia de apenas 0.72 segundos para empezar a responder (TTFT), seguido muy de cerca por Llama3 (0.86s).

C. Estabilidad y Eficiencia 

<img width="776" height="501" alt="image" src="https://github.com/user-attachments/assets/3bf86248-35d2-447c-9a15-1f79611db99d" />

Gemma:7b mostr√≥ un comportamiento irregular: fue decente en programaci√≥n (3.76 t/s) pero muy lento escribiendo texto general (1.58 t/s).

Mistral fue el modelo m√°s "estable", manteniendo su velocidad m√°xima casi id√©ntica sin importar si le ped√≠amos un poema o c√≥digo.

Consumo de Memoria: Llama3 mostr√≥ el pico de consumo de RAM m√°s alto durante la carga inicial (3.1 GB), mientras que Mistral pareci√≥ gestionar mejor los recursos una vez activo.

6. Conclusiones y Recomendaciones Finales
Tras someter a los modelos Llama3, Mistral y Gemma:7b a pruebas de rendimiento en un entorno local controlado, se extraen las siguientes conclusiones para futuros despliegues:

A. Mistral 
El modelo Mistral se posiciona como la opci√≥n m√°s robusta y eficiente para el hardware utilizado.
Por qu√©: Mantuvo la velocidad de generaci√≥n m√°s alta (6 tokens/segundo) y la mayor estabilidad a trav√©s de las distintas categor√≠as (Creatividad, Coding, Razonamiento).
Uso recomendado: Asistentes virtuales generales, generaci√≥n de contenido largo y chatbots que requieran fluidez.

B. Llama 3
Aunque ligeramente m√°s lento en generaci√≥n bruta que Mistral, Llama 3 destac√≥ por su baj√≠sima latencia de respuesta una vez cargado (apenas 0.8 - 0.9 segundos).
Por qu√©: Su arquitectura demuestra una gran optimizaci√≥n para tareas de razonamiento y resumen.
Uso recomendado: Tareas que requieran alta precisi√≥n l√≥gica, an√°lisis de texto y flujos de trabajo donde el usuario espera una respuesta inmediata (baja latencia).

C. Gemma:7b
El modelo de Google mostr√≥ un rendimiento inferior en este entorno espec√≠fico, con velocidades por debajo de los 2 tokens/segundo en tareas creativas.
Observaci√≥n: Esto sugiere que Gemma podr√≠a requerir hardware m√°s potente para competir en igualdad de condiciones con la eficiencia de Mistral.

[‚¨ÖÔ∏è Anterior](../0.8.4/0.8.4.md) | [üè† Home](../../../README.md)
